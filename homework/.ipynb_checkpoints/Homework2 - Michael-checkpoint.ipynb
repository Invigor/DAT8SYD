{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYD DAT 8 Homework 2 - Visualisation and Regression\n",
    "\n",
    "## Homework - Due Friday 30th June\n",
    "\n",
    "#### Setup\n",
    "* Signup for an AWS account\n",
    "\n",
    "#### Communication\n",
    "* Imagine you are trying to explain to someone what Linear Regression is - but they have no programming/maths experience? How would you explain the overall process, what a p-value means and what R-Squared means?\n",
    "* Read the paper [Useful things to know about machine learning]( https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). \n",
    "    * What have we covered so far from this paper? \n",
    "    * Explain sections 6-13 in your own words\n",
    "\n",
    "#### Machine Learning\n",
    "* Describe 3 ways we can select what features to use in a model\n",
    "* Complete the first 3 exercises from Chapter 3 of Introduction to Statistical Learning in Python\n",
    "\n",
    "#### Course Project\n",
    "* For the following setup a new github repository for your project and share it with Alasdair and Ian over Slack.\n",
    "* Load the data you have gathered for your project into Python and run some summary statistics over the data. Are there any interesting features of the data that jump out? (Include the code)\n",
    "* Draft/Sketch (or wireframe) some data visualisations that would be useful for you to explore your data set\n",
    "* Are there any regresion or clustering techniques you could use in your project? Write them down (with the corresponding scikit learn function) and what you think you would get out of it. Try it out if you get a chance.\n",
    "\n",
    "\n",
    "**Instructions: copy this file and append your name in the filename, e.g. Homework2_ian_hansel.ipynb.\n",
    "Then commit this in your local repository, push it to your github account and create a pull request so I can see your work. Remeber if you get stuck to look at the slides going over Fork, Clone, Commit, Push and Pull request.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AWS Sign Up\n",
    "\n",
    "Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Firstly, let's understand what linear regression is used for. It provides a mathematical formula to predict an outcome given a set of  inputs. The formula is determined by analysing previous known results based on the corresponding inputs.\n",
    "\n",
    "For example, say we wanted to determine how much a house is about to sell for at auction. We have the previous 12 months of auction results with the actual sell price as well as the corresponding details on the house. For example, how many bedrooms, how many bathrooms, how large is the block, that suburb is it in, etc.\n",
    "\n",
    "Linear regression allows us to use all of the historic auction results to create a formula to predict future auction results.\n",
    "\n",
    "The formula would take as input, the characteristics available in the historic results i.e. number of bedrooms, number of bathrooms, etc. The output of the formula would be the estimated sell price.\n",
    "\n",
    "The formula for linear regression is actually very simple. Each of the inputs (number of bathrooms, land area, etc) is multiplied by a specific factor for that input. Each of these multiplications are added together with another value added in on top which we will call the constant.\n",
    "\n",
    "The hard bit is working out what factor to use when multiplying each of the inputs. To keep it simple, let's just consider one input - the number of bedrooms.\n",
    "\n",
    "Most people would understand that in general, the more bedrooms a property has, the more it would expect to sell at auction. A really simple linear regression formula would look something like the minimum price is going to be \\$400k plus \\$200k per bedroom. So a one bedroom property is expected to sell for \\$600k while a two bedroom is expected to sell for \\$800k.\n",
    "\n",
    "In this case, the constant is \\$400k while the factor or multiplier for bedrooms is $200,000.\n",
    "\n",
    "The reason it is called Linear Regression is if you plot this formula on a graph with the number of bedrooms on the horizontal axis and price on the vertical axis, it actually looks like straight line.\n",
    "\n",
    "So how did we determine that the factor for bedrooms is \\$200k and the constant was \\$400k?\n",
    "\n",
    "We basically plot all of our know auction results on the chart with price on the vertical axis and number of bedrooms on the horizontal access. These will appear as lots of points on the chart and we then try to draw a straight line that fits best with all of our points.\n",
    "\n",
    "The mathematics behind linear regression is working out the line the best fits the points and the way it works is to minimise the distance between any point on the chart (number of bedrooms and house price) and the estimation line. To get the best fit, we add all of the individual distances together and try to find a line that makes the total as small as possible. This value is called the Root Mean Square Error (RMSE) because of the mathematical formula used to calculate it.\n",
    "\n",
    "It is very unlikely that the RMSE will be zero as this would mean our formula is able to predict house prices perfectly. Instead, it tells us how accurate model is likely to be. For example, if the RMSE is 30,000, we would expect our linear regression model to predict house prices with an accuracy of around $30k.\n",
    "\n",
    "The example above was very simple with only one input - the number of bedrooms. As indicated earlier, we generally know a lot more about houses that go to auction including the number of bedrooms, the land area and so forth. We can include these other variables into the model and linear regression model will calculate the relevant multiplication factor for each variable.\n",
    "\n",
    "In addition to the RMSE, the linear regression algorithm also gives us another value to help determine how good the model is. This is called the p-value and we actually get one for every different input variable we include in the model. \n",
    "\n",
    "The p-value tells us how relevant the input variable is to the predictive model. For example, the number of bedrooms is highly relevant to the price of the house and would therefore have a low p-value but the house colour is probably less relevant so would have a high p-value.\n",
    "\n",
    "For inputs with a high p-value (not relevant), it might be better to remove them from the model as in some cases, irrelevant variables can actually reduce the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Few Useful Things to Know about Machine Learning\n",
    "## What have we covered so far from this paper? \n",
    "* Classification: Taking a vector set of discrete or continuous feature variables as input and outputs a single discrete value known as the class.\n",
    "* k-Mean Clustering: Finding the k most similar training examples and predicting the most like class for the input variables\n",
    "* Evaluation Function: Used to quantify the quality of different models applied to the same training data\n",
    "* Test Data Set: Excluding a subset of the training data to test the resulting model after training.\n",
    "* Overfitting: Creating a model that is very accurate for the training data but inaccurate for test data. The resulting error can be divided into bias and variance.\n",
    "* Bias: The tendancy for a model to learn the same thing incorrectly. the same mistake consistently with different input data.\n",
    "* Variance: The tendancy for a model to learn random things irrespective of the training data.\n",
    "* Cross-validation: Dividing the original data sample into a training set to training the model and a test set to evaluate its accuracy.\n",
    "* Regularisation: Adding a penalisation factor to the model to favour smaller models reducing the probabilty of overfitting.\n",
    "* Feature Selection: Selecting the most informationally valuable input variables to the model noting that some may only contribute if used on combination with others.\n",
    "\n",
    "\n",
    "## Explain sections 6-13 in your own words\n",
    "\n",
    "### 6. Intuition fails in high dimensions\n",
    "\n",
    "Increasing the number of variables in a model creates a number of difficulties:\n",
    "1. As the number of variables increases, the amount of training data required to effectively cover the input space increase expontially. \n",
    "2. More variables, especially if those variables are not particularly relevant, creates 'noise' which renders nearest neighbour classifiers useless\n",
    "3. Human intuition, which is best when considering 3 or less dimensions, struggles with higher dimensions making it difficult to  understand the model and therefore iterate and improve the model\n",
    "\n",
    "Fortunately, many high dimensional phenomena we may try to model can be trained with limited training sets because they are naturally driven by a smaller number of key dimensions.   \n",
    "\n",
    "### 7. Theoretical guaranteees are not what they seem\n",
    "\n",
    "There is a theorectical number of examples needed to ensure a model that provides a good generalisation of the problem space. Unfortunately there are a number of cavaets to this theory:\n",
    "1. The upper bound can still be impractically high, although in most cases, a smaller number of examples will suffice\n",
    "2. The theory only provides a probability of good generalisation, not a guarantee, so it is still possible not to arrive a good generalisation even with a large number of examples. Models can also get 'stuck' in local minima which will provide good generalisation as well.\n",
    "\n",
    "As such the theoretical upper bound is best used as guidance and insight rather than a hard and fast rule.\n",
    "\n",
    "### 8. Feature engineering is key\n",
    "\n",
    "One of the most critical determinants of a model's performance is the selection of which features to incorporate into the model. Ideally the model incorporates independent variables which are highly correlated with the target.\n",
    "\n",
    "To get to this point, there is often a large amount of work to be done in retrieving, cleaning and pre-processing the data before the actual model can be built.\n",
    "\n",
    "Where a large number of candidate features are available there are techiques to manually assess and prioritise their inclusion in the model while there are also automated methods becoming increasingly popular. One issue to be aware of with automated techniques which analyse all candidate features simultaneously is that in some cases features may be irrelevant in isolation but valuable in combination. \n",
    "\n",
    "### 9. More data beats a cleverer algorithm\n",
    "\n",
    "If the sample data is limited, it may reduce the effectiveness of certain models. The first instinct of many data scientists would be to try another model but in most cases, it is better to try to get more sample data. Part of the reason is that many models actually work in a very similar manner or can be derived from each other.\n",
    "\n",
    "There are, however, practical issues with this approach including the availability of more sample data and the ability of computing platforms to process the additional data.\n",
    "\n",
    "If new models are to be tried, it is generally best to start with the most simple model and move to more complex models if the simpler model does not suffice. Importantly, simpler models are easier to understand and tune compared with more complex models.\n",
    "\n",
    "Utimately the main bottleneck to developing a successful model is the data scientist's ability to iterate (test and learn), especially if the time to provide a result is limited.\n",
    "\n",
    "### 10. Learn many models, not just one\n",
    "\n",
    "Different problems may respond better to different models so it is important to be conversant with a range of different techniques. New techniques are actually applying multiple models to the same training and test sets and then combining the results via a voting mechanism. For example, the winner and runner up of the Netflix prize were combinations of more than 100 leaners in an ensemble.\n",
    "\n",
    "Model ensembles should not be confused with Baysian Model Averaging (BMA) \n",
    "\n",
    "\n",
    "### 11. Simplicity does not imply accuracy\n",
    "\n",
    "There is some belief within the data science community that simpler models will deliver lower test error if it has the same training error as a more complex model. This is related to the perception that complexity can lead to overfitting, however, ensemble models generally disprove that theory as do Support Vector Machines.\n",
    "\n",
    "In some cases it may be better to choose the simpler model but this is easier to understand and optimise rather than assuming it will deliver better outcome. \n",
    "\n",
    "### 12. Representable does not imply learnable\n",
    "\n",
    "There is a difference between being able to represent a function mathematically and then being able to learn a function. As such, it may be necessary to try different learners on the same function as the methodology behind one learner may preclude it from being learned by that learner.\n",
    "\n",
    "### 13. Correlation does not imply causation\n",
    "\n",
    "Many predictive models are based on correlation between the features and target. It is convenient to assume that the features are also causally related to the target but this is not necessarily the case. This is critical when trying to apply strategies in the real world based on identified correlation within the model.\n",
    "\n",
    "Specifically, does the feature cause the target or does the target cause the feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning\n",
    "## Feature Selection\n",
    "1. Regularization\n",
    "2. Lassoo\n",
    "3. PCA\n",
    "\n",
    "## ISL Exercises\n",
    "### Question 1\n",
    "\n",
    "The p-value indicates how well the sample supports the null hypothesis i.e. is the error statistically significant or not. It helps to determine if there is a relationship between the variable of interest and the target.\n",
    "\n",
    "In the example provided in table 3.4, Intercept, TV and Radio all have p-values < 0.0001 which suggests that increasing spend in these areas will increase the sale of units. On the other hand, the p-value for newspapers is significantly higher at 0.8599 suggesting that newpaper advertising has no discernable impact on sales.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "KNN stands for K Nearest Neighbour and can be used for both unsupervised (classification) and supervised (regression) learning.\n",
    "\n",
    "KNN Classification involves randomly selecting K initial centroids and calculating the Euclidean distance from each point to each centroid. The point is then assigned to its nearest centroid and the centroid's position is recalculated based on its assigned points.\n",
    "\n",
    "This process iterates until a stopping criteria is met.\n",
    "\n",
    "KNN Regression uses a similar technique to estimate continuous variablesbased on weighted average of the k nearest neighbors, weighted by the inverse of their distance. \n",
    "\n",
    "### Question 3\n",
    "\n",
    "The model is as follows:\n",
    "\n",
    "salary = 50 + 20 x GPA + 0.7 x IQ + 35 x Gender + 0.01 x GPA x IQ - 10 x GPA x Gender\n",
    "\n",
    "#### (a)\n",
    "\n",
    "The correct answer is iii as the interaction between GPA and Gender means males earn more than males with the same IQ and GPA if the GPA is greater than 3.5.\n",
    "\n",
    "### (b)\n",
    "\n",
    "$129,440\n",
    "\n",
    "### (c)\n",
    "\n",
    "False. The coefficient is small compared with the other interaction coefficient because IQs are much higher than gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
